# Efficient QLoRA Adaptation of a Compact LLM for Python Code Generation

## 7043SCN - Generative AI and Reinforcement Learning - 2526JANMAR

This project implements efficient fine-tuning of a compact Large Language Model (LLM) for Python code generation using QLoRA (Quantized Low-Rank Adaptation). QLoRA enables parameter-efficient fine-tuning by freezing the base model weights and training low-rank adapter layers.

---

## Project Objectives

* Fine-tune a compact LLM using QLoRA
* Enable efficient Python code generation
* Reduce memory usage using quantization
* Evaluate model performance

---

## Technologies Used

* Python
* HuggingFace Transformers
* QLoRA
* PyTorch
* Google Colab

---

## Model Architecture

The project uses a Transformer-based Large Language Model with QLoRA adapters for efficient fine-tuning.

---

## Author
KOMAL KUMAR KARAMALA 
16364251
MSC DATA SCIENCE AND COMPUTATIONAL INTELLIGENCE
